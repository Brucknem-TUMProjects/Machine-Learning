%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../Ressources/Preamble.tex} % !!! DON'T TOUCH !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\ExerciseNumber}{01}

\newcommand{\PersonOne}{Marcel Bruckner (03674122)}
\newcommand{\PersonTwo}{Julian Hohenadel (03673879)}
\newcommand{\PersonThree}{Kevin Bein (03707775)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOKUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../Ressources/Cover.tex} % !!! DON'T TOUCH !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% !!! HOMEWORK STARTS HERE !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\Topic{Linear Algebra}
%
\Problem{1}
Dimensions of matrices $A, B, C, D, E, F$
%
\begin{align}
	A \in \Matrix{M}{N}, \qquad B &\in \Matrix{1}{M}, \qquad C \in \Matrix{N}{P}\\
	D \in \Vector{Q}, \qquad B &\in \Matrix{N}{N}, \qquad C \in \Vector{1}
\end{align}%
%
\Problem{2}
$f(x) = \sum_{i=1}^{N}\sum_{j=1}^{N}x_ix_jM_{ij}$ using only matrix-vector multiplications.
%
\begin{align}
	f(x) = x^TMx
\end{align}
%
%
\Problem{3}
\begin{enumerate}[label=(\alph*)]
	\item Conditions for unique solution $x$ for any choice of $b$ in $Ax=b$
	\subitem $rank(A)=M,\qquad det(A) \neq 0,\qquad ker(A) = \{0\}$

	\item Unique solution $x$ for any choice of $b$ in $Ax=b$ with eigenvalues of A: $\{-5,0,1,1,3\}$
	\subitem $det(A) = \prod_{i}\lambda_i = -5*0*1*1*3 = 0 \implies$ No unique solution
\end{enumerate}
%
%
\Problem{4}
Properties of eigenvalues of $A$ in $BA = AB = I$
%
\begin{align}
BA = AB = I \implies B = A^{-1}
\end{align}
A has to be invertable $\implies det(A) \neq 0 \implies \forall i: \lambda_i \neq 0$
%
\Problem{5}
$A$ is PSD if and only if it has no negative eigenvalues\\
%
Definition of eigenvalue: $Ax = \lambda x$
%
\begin{align}
	PSD &\Leftrightarrow x^TAx \geq 0\\
	PSD &\Leftrightarrow x^TAx = x^T \lambda x = \lambda x^T x = \lambda \sum_{i} x_i^2 \geq 0\\
	\sum_{i} x_i^2 &\geq_{always} 0 \implies \forall \lambda: \lambda \geq 0
\end{align}
%
%
\Problem{6}
$B = A^T A$ is PSD for any $A$
%
\begin{align}
	B = A^T A &\implies Bx = \lambda_B x = A^T A x = \lambda_A \lambda_A x = \lambda_A^2 x\\
	\lambda_B = \lambda_A^2 &\implies \lambda_B \geq_{always} 0
\end{align}
B has to be PSD for any choice of A. \qquad \ensuremath{\square}









\Topic{Calculus}
%
\Problem{7}

\begin{enumerate}[label=(\alph*)]
	\item Under what conditions does this optimization problem have (i) a unique solution, (ii) infinitely many solutions or (iii) no solution? Justify your answer.
	\begin{enumerate}[label=(\roman*)]
		\item The function got a global minimum. $\implies a > 0$ 
		\item The function got infinite local minima. $\implies a = b = 0$
		\item The function is not bounded below. $\implies a < 0$
	\end{enumerate}

	\item Assume that the optimization problem has a unique solution. Write down the closed-form expression for $x^\star$ that minimizes the objective function.
		\subitem $f^{\prime}(x)\stackrel{!}{=}0 \\ f^{\prime}(x) = ax + b = 0 \\ x^\star = \underset{x\in \mathbb{R}}{\operatorname{argmin}}f(x)= \frac{-b}{a}  $
\end{enumerate}
%
%
\Problem{8}
\begin{enumerate}[label=(\alph*)]
	\item Compute the Hessian $\nabla^{2} g(x)$ of the objective function. Under what conditions does this optimization problem have an unique solution?
		\subitem $ g(x) = \frac{1}{2}\begin{bmatrix} x_{1} & x_{2} & \hdots & x_{n}  \end{bmatrix}A\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix}
			+ b^{T}\begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} + c \\
				g(x) = \frac{1}{2} \sum_{i}x_{i}\sum_{j}A_{ij}x_{j} + \sum_{i}(b^{T})_{i}x_{i} + c \implies g^{\prime\prime}(x) = \begin{cases} A_{ij},&\text{if } i = j\\ 0 & \text{if } i \neq j\end{cases} \\
			\implies \nabla^{2} g(x) = \begin{bmatrix} A_{11} & 0 & \hdots & 0 \\ 0 & A_{22} & \hdots & \vdots \\ \vdots & & \ddots & 0 \\ 0 & \hdots & 0 & A_{nn} \end{bmatrix}$ \\
			\\
			Unique solution only if: $\forall i: A_{ii}\neq 0 \implies det(A) \neq 0$.

	\item Why is it necessary for a matrix \textbf{A} to be PSD for the optimization problem to be well-defined? What happens if A has a negative eigenvalue?
		\subitem $g(x) = \frac{1}{2} x^{T}Ax + b^{T} + c\\ g(x) = \frac{1}{2} x^{T}\lambda_A  x + b^{T}x + c \\ g(x)= \frac{1}{2} \lambda_A \sum_{i}x_{i}^{2} + \sum_{i}b_{i}^{T}x_{i} + c \\ g^{\prime\prime}(x) = \lambda \implies$ curvature same in all directions $\implies$ global minimum $\implies$ convex problem \\ $\implies$ negative EV $\implies$ only sattle point

\item Assume that the matrix \textbf{A} is positive definite (PD). Write doen the closed-form expression for $x^\star$ that minimizes the objective function.
	\subitem $x^\star = \underset{x\in \mathbb{R^{N}}}{\operatorname{argmin}} \implies (x)=g^{\prime}(x)\stackrel{!}{=}0 \\ g^{\prime}(x)= \frac{\partial }{\partial x}(\frac{1}{2} x^{T}Ax + b^{T} + c) \\ g^{\prime}(x)= \frac{1}{2}x^{T}A \frac{\partial x}{\partial x} + b^{T} \frac{\partial x}{\partial x} + c \frac{\partial 1}{\partial x} \\ g^{\prime}(x) = \frac{1}{2}x^{T}A+b^{T} + 0 \implies \\ $\begin{align*} \frac{1}{2}x^{T}A + b^{T} &= 0 \\ x^{T}A &= -2b^{T} \\ x^{T} &= -2b^{T}A^{-1} \\ x &= (-2b^{T}A^{-1})^{T} \end{align*}
\end{enumerate}







\Topic{Probability Theory}
%
\Problem{9}
\textit{Missing counter example}
%
\Problem{10}
\[ P(A) = P(A|B) \overset{Bayes}{=} \frac{P(A,B)}{P(B)} = \frac{P(A)P(B)}{P(B)} = P(A) \]
\[P(A|B,C) = \frac{P(A,B|C)}{P(B|C)} = \frac{P(A|C)P(B|C)}{P(B|C)} = P(A|C) \]
%
%
\Problem{11}
\[ p(a) = \int \int p(a,b,c)\,db\,dc \]
\[ p(c|a,b) = \frac{p(a,b,c)}{p(a,b)} = \frac{p(a,b,c)}{\int p(a,b,c)\, dc} \]
\[ p(b|c) = \frac{p(b,c)}{p(c)} = \frac{\int p(a,b,c) \, da}{\int \int p(a,b,c)\,da\,db} \]
%
%
\Problem{12}
\begin{flushleft}
Extracting the variables from the Text:
\end{flushleft}
\begin{table}[h]
\begin{tabular}{lllll}
  $P(T) :=$ Test positive & $P(S) :=$ sick & $P(T|S) = 0.95$ & $P(\neg T|\neg S) = 0.95$ & $P(S) = \frac{1}{1000} = 0.001$ \\
  $P(\neg T) :=$ Test negative & $P(\neg S) :=$ healthy & $P(T|\neg S) = 0.05$ & $P(\neg T|S) = 0.05$ & $P(\neg S) = 0.999$
\end{tabular}
\end{table}
\begin{flushleft}
Calculation of $P(S|T)$:
\end{flushleft}
\begin{align*}
  P(S|T) &= \frac{P(T|S)P(S)}{P(T|S)P(S) + P(T|\neg S)(\neg S)} &\\
  &= \frac{0.95 \cdot 0.00.1}{0.95 \cdot 0.001 + 0.05 \cdot 0.999} &\\
  &\approx 0.019 &
\end{align*}
%
%
\Problem{13}
\begin{flushleft}
Given
\begin{itemize}
  \item $\E[x-\mu] = \E[\mu] = \mu - \mu = 0$
  \item $Var[x] = \sigma^2 = \mathbb[(x-\mu)^2]a$
\end{itemize}
the expected values of $f(x)$ becomes the following:
\end{flushleft}
\begin{align}
  \E[f(x)] &= \E[ax + bx^2 + c] &\\
  &= \E[ax] + \E[bx^2] + \E[c] &\\
  &= a \E[x] + b \E[x^2] + c &\\
  &= a\mu + b \E[(x-\mu+\mu)^2] + c &\\
  %&= a\mu + c + b(\E[(\overset{:= \phi}{(x-\mu)} + \mu) (\overset{:= \phi}{(x-\mu)}+\mu)]) &\\
  &= a\mu + c + b(\E[((x-\mu) + \mu) ((x-\mu)+\mu)]) &\phi := x-\mu \\
  &= a\mu + c + b(\E[(\phi + \mu)^2]) \\
  &= a\mu + c + b(\E[\phi^2 + 2\phi\mu +\mu^2]) &\\
  &= a\mu + c + b(\E[\phi^2] + \E[2\phi\mu] + \E[\mu^2]) &\\
  &= a\mu + c + b(\E[(x-\mu)^2] + 2\mu \E[x-\mu] + \E[\mu^2]) &\\
  &= a\mu + c + b(\sigma^2 + 0 + \mu^2) &\\
  &= a\mu + c + b\sigma^2 + b\mu^2 &
\end{align}
%
%
\Problem{14}
\begin{itemize}
  \item
    \begin{align*}
    \E[g(x)] &= \E[Ax] &\\
    &= A\E[x] &\\
    &= A\mu &
  \end{align*}
  \item
    \begin{align*}
      \E[g(x)g(x)^T] &= \E[Ax(Ax)^T] &\\
      &= A \E[x(ax)^T] &\\
      &= A \E[xx^TA^T] &\\
      &= A \E[xx^T] A^T &\\
      &= A(\Sigma + \mu\mu^T)A^T &\\
      &= A A^T \Sigma + AA^T\mu\mu^T&
    \end{align*}
  \item
    \begin{align*}
      \E[g(x)^Tg(x)] &= \E[(Ax)^TAx] &\\
      &= \E[x^TA^TAx] & B := A^TA \\
      &= \E[\sum_{i=1}^N \sum_{j=1}^N B_{i,j} x_i x_j ] & \\
      &= \sum_{i=1}^N \sum_{j=1}^N B_{i,j} \E[x_ix_j] &\\
      &= \sum_{i=1}^N \sum_{j=1}^N B_{i,j} (\sigma_{i,j} + \mu_i \mu_j) &\\
      &= \sum_{i=1}^N \sum_{j=1}^N B_{i,j} \sigma_{i,j} + \sum_{i=1}^N \sum_{j=1}^N B_{i,j} \mu_i \mu_j &\\
      &= \sum_{i=1}^N (B\Sigma)_{i,i} = \mu^T B \mu &\\
      &= tr(A^TA\Sigma) + \mu^TA^TA\mu &
    \end{align*}
  \item
    \begin{align*}
      Cov[g(x)] &= Cov[Ax] &\\
      &= \E[(Ax - \E[Ax])(Ax - \E[Ax])^T] &\\
      &= \E[(Ax - A\E[x])(Ax - A\E[x])^T] &\\
      &= \E[A(x - A\E[x])(x - \E[x])^TA^T] &\\
      &= A\E[(x - A\E[x])(x - \E[x])^T]A^T &\\
      &= A \, Cov(x) A^T &
    \end{align*}
\end{itemize}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% !!! HOMEWORK ENDS HERE !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../Ressources/Appendix.tex} % !!! DON'T TOUCH !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
