%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../Ressources/Preamble.tex} % !!! DON'T TOUCH !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\ExerciseNumber}{08}

\newcommand{\PersonOne}{Marcel Bruckner (03674122)}
\newcommand{\PersonTwo}{Julian Hohenadel (03673879)}
\newcommand{\PersonThree}{Kevin Bein (03707775)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOKUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../Ressources/Cover.tex} % !!! DON'T TOUCH !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% !!! HOMEWORK STARTS HERE !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\Topic{SVM and Kernels}
%
\Problem{1}
%
\begin{flushleft}
Similarities: \\
Both try to find a fitting hyperplane which seperates the data classes.\\
Difference: \\
SVM tries to maximize the margin from the hyperplane to the data points, perceptron algorithms only care about a valid seperation of the data classes.
\end{flushleft}
%
%
\Problem{2}
%
\begin{flushleft}
a)\\
$g(\alpha)$ vectorized definition:
\begin{align*}
g(\alpha) = \frac{1}{2}\alpha^{T}Q\alpha + \alpha^{T}1_{N}
\end{align*}
$g(\alpha)$ standard definition:
\begin{align*}
g(\alpha) = \sum_{i=1}^{N}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}
y_{i}y_{j}\alpha_{i}\alpha_{j}x^{T}_{i}x_{j}
\end{align*}
$y$ is a vector of dimension $N\times 1$\\
$x$ is a matrix of dimension $N\times M$\\
$\sum_{i=1}^{N}\sum_{j=1}^{N}y_{i}y_{j}$ is equivalent to $yy^{T}$ 
(dimension is $N\times N$)\\
$\sum_{i=1}^{N}\sum_{j=1}^{N}x^{T}_{i}x_{j}$ is equivalent to $XX^{T}$ 
(dimension is $N\times N$)\\
$\sum_{i=1}^{N}\sum_{j=1}^{N}y_{i}y_{j}x^{T}_{i}x_{j}$ is the Hadamard product so:
$[yy^{T} \odot XX^{T}]$\\
Take the $-1$ scalar from the standard definition into the matrix: 
$[-yy^{T} \odot XX^{T}] = Q$\\
$\implies \frac{1}{2}\alpha^{T}Q\alpha \equiv \frac{1}{2}\alpha^{T}[-yy^{T} 
\odot XX^{T}]\alpha \equiv -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}y_{i}y_{j}\alpha_{i}
\alpha_{j}x^{T}_{i}x_{j}$\\
$\alpha^{T}1_{N} \equiv \sum_{i=1}^{N}\alpha_{i}$ is trivial.\\
$\implies$ $g(\alpha)$ vectorized definition $\equiv$ $g(\alpha)$ standard definition
\end{flushleft}
\begin{flushleft}
b)\\
If the search for a local maximizer of $g$ returns the global maximum of $g$, that
means that the maximization problem is concave.\\
To proove this claim $Q$ needs to be negativ (semi)definite (NSD).\\
For $Q$ to be NSD: $\forall \alpha \in \R^{N}: \alpha^{T}Q\alpha \leq 0$ 
needs to hold.\\
\begin{align}
\alpha^{T}Q\alpha &\leq 0\\
-\sum_{i=1}^{N}\sum_{j=1}^{N}y_{i}y_{j}\alpha_{i}\alpha_{j}x^{T}_{i}x_{j} &\leq 0\\
-\sum_{i=1}^{N}\sum_{j=1}^{N}(y_{i}\alpha_{i}x_{i})^{T}(y_{j}\alpha_{j}x_{j}) &\leq 0\\
-(y\odot\alpha)^{T}XX^{T}(y\odot\alpha) &\leq 0\\
-(y\odot\alpha)^{T}(X^{T})^{T}X^{T}(y\odot\alpha) &\leq 0\\
-(X^{T}(y\odot\alpha))^{T}(X^{T}(y\odot\alpha)) &\leq 0\\
-(X^{T}(y\odot\alpha))^{2} &\leq 0\\
-(\geq 0) &\leq 0 \qquad \square
\end{align}
(3): $y_{i}$ and $\alpha_{i}$ can be dragged inside here because they are scalars.\\
(4): The whole expression returns a scalar, that's why reshaping is done this way:\\
$dim((y\odot\alpha)^{T})= 1 \times N$\\
$dim(X)= N \times 1$\\
$dim(X^{T})= 1 \times N$\\
$dim((y\odot\alpha))= N \times 1$\\
(5): $X=(X^{T})^{T}$\\
(6): $(AB)^{T}=B^{T}A^{T}$\\
(7): $(...)^{2} \geq 0 $, (as long as $...$ is not complex)\\
(8): Proofs that $Q$ is NSD.\\
$Q$ is NSD, that means the maximization problem is in fact concave so local maxima = global maxima.
\end{flushleft}
%
%
\Problem{3}
%
%
%
\Problem{4}
%
%
%
\Problem{5}
%
\begin{flushleft}
"A kernel is valid if it corresponds to an inner product in some feature space."\\
$x_{1}^{T}x_{2}$ is a valid kernel because it is a scalar product of the input vectors.\\
$a_{0}$ is a constant term and a valid kernel because it can be generated by:\\
$k(x_i,x_j) = \phi(x_i)^T\phi(x_j)$ with $\phi(x) = \sqrt{a_0} \implies k(x_i,x_j) = a_0$\\
The same holds for $a_i$ so $a_i(x_i^T x_j)^i + a_0$ can be represented as kernels.\\
Sums and multiplications of kernels are kernel preserving (also scalars are $\geq 0$)\\
$\implies$ Gram matrix is still PSD.
\end{flushleft}
%
%
\Problem{6}
%
\begin{flushleft}
The "trick" for this Problem:\\
"The Maclaurin series for $\frac{1}{1-x}$ is the geometric series:
$1 + x + x^2 + x^3 + x^4 ...$"
\begin{align*}
k(x_1,x_2)&=\frac{1}{1-x_1 x_2}\\
&= \sum_{i=0}^{\infty}x_1^i x_2^i\\
&= \phi(x_1)^T \phi(x_2)\\
\text{with } \phi(x)&=(1,x,x^2,x^3,x^4, ...)
\end{align*}
\end{flushleft}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% !!! HOMEWORK ENDS HERE !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../Ressources/Appendix.tex} % !!! DON'T TOUCH !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
