{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "exercise_10_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ausORJSYjq9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgU29WyvmvJ8",
        "colab_type": "code",
        "outputId": "cc4d3053-4953-4fac-a922-592c2acb7b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec 29 16:48:07 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    33W / 250W |    735MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrILyy8Wjq9f",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch\n",
        "In this notebook you will gain some hands-on experience with [PyTorch](https://pytorch.org/), one of the major frameworks for deep learning. To install PyTorch run `conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`, with cudatoolkit set to whichever CUDA version you have installed. You can check this by running `nvcc --version`. If you do not have an Nvidia GPU you can run `conda install pytorch torchvision cpuonly -c pytorch` instead. However, in this case we recommend using [Google Colab](https://colab.research.google.com/).\n",
        "\n",
        "You will start by re-implementing some common features of deep neural networks (dropout and batch normalization) and then implement a very popular modern architecture for image classification (ResNet) and improve its training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXfEqlgHjq9m",
        "colab_type": "text"
      },
      "source": [
        "# 1. Dropout\n",
        "Dropout is a form of regularization for neural networks. It works by randomly setting activations (values) to 0, each one with equal probability `p`. The values are then scaled by a factor $\\frac{1}{1-p}$ to conserve their mean.\n",
        "\n",
        "Dropout effectively trains a pseudo-ensemble of models with stochastic gradient descent. During evaluation we want to use the full ensemble and therefore have to turn off dropout. Use `self.training` to check if the model is in training or evaluation mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx3KqAOPjq9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Dropout, as discussed in the lecture and described here:\n",
        "    https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
        "    \n",
        "    Args:\n",
        "        p: float, dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, p):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        The module's forward pass.\n",
        "        This has to be implemented for every PyTorch module.\n",
        "        PyTorch then automatically generates the backward pass\n",
        "        by dynamically generating the computational graph during\n",
        "        execution.\n",
        "        \n",
        "        Args:\n",
        "            input: PyTorch tensor, arbitrary shape\n",
        "\n",
        "        Returns:\n",
        "            PyTorch tensor, same shape as input\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            mask = np.random.random(input.shape)\n",
        "            return torch.from_numpy(np.where(mask <= self.p, 0, (1 / (1 - self.p))))\n",
        "        \n",
        "        # TODO: Set values randomly to 0."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmhOVcErjq9x",
        "colab_type": "code",
        "outputId": "a071395a-e745-47cd-b332-1f9d8aecc3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Test dropout\n",
        "test = torch.ones(10_000)\n",
        "dropout = Dropout(0.5)\n",
        "test_dropped = dropout(test)\n",
        "\n",
        "print(test_dropped.sum().item())\n",
        "print((test_dropped > 0).sum().item())\n",
        "\n",
        "print(np.isclose(test_dropped.sum().item(), 10_000, atol=400))\n",
        "print(np.isclose((test_dropped > 0).sum().item(), 5_000, atol=200))\n",
        "\n",
        "# These assertions can in principle fail due to bad luck, but\n",
        "# if implemented correctly they should almost always succeed.\n",
        "assert np.isclose(test_dropped.sum().item(), 10_000, atol=400)\n",
        "assert np.isclose((test_dropped > 0).sum().item(), 5_000, atol=200)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10006.0\n",
            "5003\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBmu3c4kjq95",
        "colab_type": "text"
      },
      "source": [
        "# 2. Batch normalization\n",
        "Batch normalization is a trick used to smoothen the loss landscape and improve training. It is defined as the function\n",
        "$$y = \\frac{x - \\mu_x}{\\sigma_x + \\epsilon} \\cdot \\gamma + \\beta$$,\n",
        "where $\\gamma$ and $\\beta$ and learnable parameters and $\\epsilon$ is a some small number to avoid dividing by zero. The Statistics $\\mu_x$ and $\\sigma_x$ are taken separately for each feature. In a CNN this means averaging over the batch and all pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSOKPE3Ojq97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Batch normalization, as discussed in the lecture and similar to\n",
        "    https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d\n",
        "    \n",
        "    Only uses batch statistics (no running mean for evaluation).\n",
        "    Batch statistics are calculated for a single dimension.\n",
        "    Gamma is initialized as 1, beta as 0.\n",
        "    \n",
        "    Args:\n",
        "        num_features: Number of features to calculate batch statistics for.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        \n",
        "        # TODO: Initialize the required parameters\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features)).unsqueeze(0).unsqueeze(-1)\n",
        "        #self.gamma = self.gamma.\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features)).unsqueeze(0).unsqueeze(-1)\n",
        "        #self.beta = self.beta.\n",
        "        \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Batch normalization over the dimension C of (N, C, L).\n",
        "        \n",
        "        Args:\n",
        "            input: PyTorch tensor, shape [N, C, L]\n",
        "            \n",
        "        Return:\n",
        "            PyTorch tensor, same shape as input\n",
        "        \"\"\"\n",
        "        eps = 1e-5\n",
        "\n",
        "        mean = input.mean(dim=[0, 2], keepdim=True)\n",
        "        input_mean_norm = input - mean\n",
        "        var = torch.sqrt(input.var(dim=[0, 2],  keepdim=True))\n",
        "\n",
        "        return (input_mean_norm / (var + eps)) * self.gamma + self.beta\n",
        "        \n",
        "        # TODO: Implement the required transformation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNYot6Xyjq-J",
        "colab_type": "code",
        "outputId": "354091c1-1367-482a-c852-77f8229d6e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "source": [
        "# Tests the batch normalization implementation\n",
        "torch.random.manual_seed(42)\n",
        "test = torch.randn(8, 2, 4)\n",
        "\n",
        "b1 = BatchNorm(2)\n",
        "test_b1 = b1(test)\n",
        "\n",
        "b2 = nn.BatchNorm1d(2, affine=False, track_running_stats=False)\n",
        "test_b2 = b2(test)\n",
        "\n",
        "print(test_b1)\n",
        "print(\"-----\")\n",
        "print(test_b2)\n",
        "\n",
        "print(torch.allclose(test_b1, test_b2, rtol=0.02))\n",
        "assert torch.allclose(test_b1, test_b2, rtol=0.02)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.6380,  1.2470,  0.7253, -1.9484],\n",
            "         [ 0.6971, -1.2133, -0.0234, -1.5829]],\n",
            "\n",
            "        [[-0.7447,  1.3905, -0.4249, -1.3242],\n",
            "         [-0.7073, -0.5391, -0.7482,  0.7810]],\n",
            "\n",
            "        [[ 1.3849, -0.2178, -0.5182,  0.3152],\n",
            "         [-0.7375,  1.0964,  0.8193,  1.6979]],\n",
            "\n",
            "        [[ 1.0618,  1.0772,  0.4671,  1.1113],\n",
            "         [-0.2117,  0.0613, -0.2317,  0.8782]],\n",
            "\n",
            "        [[-1.3073, -0.8507, -0.2745,  1.4516],\n",
            "         [ 0.3380, -0.4044,  0.3249, -0.7540]],\n",
            "\n",
            "        [[-1.4611,  0.8097, -0.8583, -0.6105],\n",
            "         [-1.2529,  2.1395, -1.2134, -0.4677]],\n",
            "\n",
            "        [[-0.8886, -0.6611, -0.0064,  0.3918],\n",
            "         [-0.4678,  1.2093, -0.7933, -0.7154]],\n",
            "\n",
            "        [[-1.3238, -0.0438, -0.1323,  0.5251],\n",
            "         [-0.0781,  1.8616, -1.1634,  1.4012]]], grad_fn=<AddBackward0>)\n",
            "-----\n",
            "tensor([[[ 1.6642,  1.2669,  0.7369, -1.9796],\n",
            "         [ 0.7082, -1.2327, -0.0238, -1.6083]],\n",
            "\n",
            "        [[-0.7567,  1.4128, -0.4317, -1.3454],\n",
            "         [-0.7186, -0.5477, -0.7602,  0.7935]],\n",
            "\n",
            "        [[ 1.4070, -0.2212, -0.5265,  0.3202],\n",
            "         [-0.7493,  1.1140,  0.8324,  1.7251]],\n",
            "\n",
            "        [[ 1.0788,  1.0945,  0.4746,  1.1291],\n",
            "         [-0.2151,  0.0622, -0.2354,  0.8923]],\n",
            "\n",
            "        [[-1.3282, -0.8643, -0.2789,  1.4748],\n",
            "         [ 0.3434, -0.4109,  0.3301, -0.7660]],\n",
            "\n",
            "        [[-1.4845,  0.8227, -0.8720, -0.6202],\n",
            "         [-1.2729,  2.1737, -1.2328, -0.4752]],\n",
            "\n",
            "        [[-0.9028, -0.6717, -0.0065,  0.3981],\n",
            "         [-0.4753,  1.2287, -0.8060, -0.7269]],\n",
            "\n",
            "        [[-1.3450, -0.0445, -0.1344,  0.5335],\n",
            "         [-0.0794,  1.8914, -1.1820,  1.4237]]])\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1re5WSV4jq-Q",
        "colab_type": "text"
      },
      "source": [
        "# 3. ResNet\n",
        "ResNet is the model that first introduced residual connections (a form of skip connections). It is a rather simple, but successful and very popular architecture. In this part of the exercise we will re-implement it step by step.\n",
        "\n",
        "Note that there is also an [improved version of ResNet](https://arxiv.org/abs/1603.05027) with optimized residual blocks. Here we will implement the [original version](https://arxiv.org/abs/1512.03385) for CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDeaiLEjjq-S",
        "colab_type": "text"
      },
      "source": [
        "This is just a convenience function to make e.g. `nn.Sequential` more flexible. It is e.g. useful in combination with `x.squeeze()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnFryWDyjq-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIbTr7Gbjq-e",
        "colab_type": "text"
      },
      "source": [
        "We begin by implementing the residual blocks. The block is illustrated by this sketch:\n",
        "\n",
        "![Residual connection](attachment:residual_connection.png)\n",
        "\n",
        "Note that we use 'SAME' padding, no bias, and batch normalization after each convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZi8mzf9jq-g",
        "colab_type": "code",
        "outputId": "fa8f195b-d3fd-4298-d3c9-23afb87db96d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The residual block used by ResNet.\n",
        "    \n",
        "    Args:\n",
        "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
        "        out_channels: The number of channels after the first convolution\n",
        "        stride: Stride size of the first convolution, used for downsampling\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()        \n",
        "        if stride > 1 or in_channels != out_channels:\n",
        "            # Add strides in the skip connection and zeros for the new channels.\n",
        "            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride],\n",
        "                                               (0, 0, 0, 0, 0, out_channels - in_channels),\n",
        "                                               mode=\"constant\", value=0))\n",
        "        else:\n",
        "            self.skip = nn.Sequential()\n",
        "            \n",
        "        # TODO: Initialize the required layers\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: Execute the required layers and functions\n",
        "\n",
        "        # Skip connection = identity\n",
        "        identity = self.skip(input)\n",
        "\n",
        "        # conv - bn - relu\n",
        "        out = self.conv1(input)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # conv - bn - skip - relu\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "test_ResidualBlock = ResidualBlock(3,16)\n",
        "print(test_ResidualBlock)\n",
        "# Skip is not correct, i guess"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResidualBlock(\n",
            "  (skip): Lambda()\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjh3jd8-jq-m",
        "colab_type": "text"
      },
      "source": [
        "Next we implement a stack of residual blocks for convenience. The first layer in the block is the one changing the number of channels and downsampling. You can use `nn.ModuleList` to use a list of child modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfrSglELjq-o",
        "colab_type": "code",
        "outputId": "0d46b841-2069-49eb-f629-e55969ec3727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "class ResidualStack(nn.Module):\n",
        "    \"\"\"\n",
        "    A stack of residual blocks.\n",
        "    \n",
        "    Args:\n",
        "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
        "        out_channels: The number of channels after the first layer\n",
        "        stride: Stride size of the first layer, used for downsampling\n",
        "        num_blocks: Number of residual blocks\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
        "        super().__init__()\n",
        "        \n",
        "        # TODO: Initialize the required layers (blocks)\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        # Init ModuleList\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # First block is used for downsampling\n",
        "        self.layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "\n",
        "        # Append n - 1 blocks:\n",
        "        # block: conv - bn - relu - conv - bn - skip - relu\n",
        "        for i in range(0, self.num_blocks - 1):\n",
        "            self.layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        \n",
        "    def forward(self, input):\n",
        "        # TODO: Execute the layers (blocks)\n",
        "        for block in self.layers:\n",
        "            out = self.layers[block](out)\n",
        "        return out\n",
        "\n",
        "test_ResidualStack = ResidualStack(3,32,2,5)\n",
        "print(test_ResidualStack)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResidualStack(\n",
            "  (layers): ModuleList(\n",
            "    (0): ResidualBlock(\n",
            "      (skip): Lambda()\n",
            "      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (skip): Sequential()\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (skip): Sequential()\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (skip): Sequential()\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (skip): Sequential()\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_-foZfsjq-u",
        "colab_type": "text"
      },
      "source": [
        "Now we are finally ready to implement the full model! To do this, use the `nn.Sequential` API and carefully read the following paragraph from the paper (Fig. 3 is not important):\n",
        "\n",
        "![ResNet CIFAR10 description](attachment:resnet_cifar10_description.png)\n",
        "\n",
        "Note that a convolution layer is always convolution + batch norm + activation (ReLU), that each ResidualBlock contains 2 layers, and that you might have to `squeeze` the embedding before the dense (fully-connected) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqZSVGABjq-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d2cc017-101e-4874-f16e-06b726a48da8"
      },
      "source": [
        "n = 5\n",
        "num_classes = 10\n",
        "\n",
        "# TODO: Implement ResNet via nn.Sequential\n",
        "\n",
        "    # Input: 32 x 32 x 3 \n",
        "        # First layer: conv 3 x 3\n",
        "        # Stack: Size=6n\n",
        "            # conv 3 x 3\n",
        "            # Feature map {32, 16, 8}\n",
        "                # foreach feature map size: 2n layers\n",
        "                # number of filters: {16, 32, 64}\n",
        "        # subsampling with convs (stride=2)\n",
        "        # Last layers:\n",
        "            # avg pooling\n",
        "            # 10-way fc\n",
        "            # softmax\n",
        "\n",
        "\"\"\"\n",
        "    _____________________________________________\n",
        "\n",
        "    output map size | 32 x 32 | 16 x 16 | 8 x 8 |\n",
        "    _____________________________________________\n",
        "        # layers    |  1 + 2n |    2n   |   2n  |\n",
        "\n",
        "        # filters   |    16   |    32   |   64  |\n",
        "\"\"\"\n",
        "\n",
        "resnet = nn.Sequential(\n",
        "    ResidualStack(3,32,1,1),\n",
        "    ResidualStack(32,32,2,2*n),\n",
        "    ResidualStack(32,32,2,2*n),\n",
        "    ResidualStack(16,8,2,2*n),\n",
        "    nn.AdaptiveAvgPool2d(None),\n",
        "    nn.Linear(64, num_classes),\n",
        "    nn.Softmax(num_classes)\n",
        ")\n",
        "\n",
        "print(resnet)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): ResidualStack(\n",
            "    (layers): ModuleList(\n",
            "      (0): ResidualBlock(\n",
            "        (skip): Lambda()\n",
            "        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (1): ResidualStack(\n",
            "    (layers): ModuleList(\n",
            "      (0): ResidualBlock(\n",
            "        (skip): Lambda()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (6): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (7): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (8): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (9): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (2): ResidualStack(\n",
            "    (layers): ModuleList(\n",
            "      (0): ResidualBlock(\n",
            "        (skip): Lambda()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (6): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (7): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (8): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (9): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (3): ResidualStack(\n",
            "    (layers): ModuleList(\n",
            "      (0): ResidualBlock(\n",
            "        (skip): Lambda()\n",
            "        (conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (6): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (7): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (8): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (9): ResidualBlock(\n",
            "        (skip): Sequential()\n",
            "        (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (4): AdaptiveAvgPool2d(output_size=None)\n",
            "  (5): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (6): Softmax(dim=10)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSOYxIRqjq-3",
        "colab_type": "text"
      },
      "source": [
        "Next we need to initialize the weights of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAa4ij7Gjq-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weight(module):\n",
        "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "    elif isinstance(module, nn.BatchNorm2d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "        \n",
        "resnet.apply(initialize_weight);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cw1V2Pijq-_",
        "colab_type": "text"
      },
      "source": [
        "# 4. Training\n",
        "So now we have a shiny new model, but that doesn't really help when we can't train it. So that's what we do next.\n",
        "\n",
        "First we need to load the data. Note that we split the official training data into train and validation sets, because you must not look at the test set until you are completely done developing your model and report the final results. Some people don't do this properly, but you should not copy other people's bad habits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sdoS13cjq_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CIFAR10Subset(torchvision.datasets.CIFAR10):\n",
        "    \"\"\"g at the same time, the operation becomes significantly cheaper computationally. If you use stride=1 \n",
        "    Get a subset of the CIFAR10 dataset, according to the passed indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, idx=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        \n",
        "        if idx is None:\n",
        "            return\n",
        "        \n",
        "        self.data = self.data[idx]\n",
        "        targets_np = np.array(self.targets)\n",
        "        self.targets = targets_np[idx].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz7qfojgjq_H",
        "colab_type": "text"
      },
      "source": [
        "We next define transformations that change the images into PyTorch tensors, standardize the values according to the precomputed mean and standard deviation, and provide data augmentation for the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H50hTjFjq_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, 4),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ6kqARNjq_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "06bd62ac-57e6-4533-9424-46152d9ed0d4"
      },
      "source": [
        "ntrain = 45_000\n",
        "train_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain),\n",
        "                          download=True, transform=transform_train)\n",
        "val_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain, 50_000),\n",
        "                        download=True, transform=transform_eval)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform_eval)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd2ooW9Bjq_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloaders = {}\n",
        "dataloaders['train'] = torch.utils.data.DataLoader(train_set, batch_size=128,\n",
        "                                                   shuffle=True, num_workers=2,\n",
        "                                                   pin_memory=True)\n",
        "dataloaders['val'] = torch.utils.data.DataLoader(val_set, batch_size=128,\n",
        "                                                 shuffle=False, num_workers=2,\n",
        "                                                 pin_memory=True)\n",
        "dataloaders['test'] = torch.utils.data.DataLoader(test_set, batch_size=128,\n",
        "                                                  shuffle=False, num_workers=2,\n",
        "                                                  pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7HlEMuSjq_b",
        "colab_type": "text"
      },
      "source": [
        "Next we push the model to our GPU (if there is one)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oGToxTSjq_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "resnet.to(device);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnJIcwDjq_j",
        "colab_type": "text"
      },
      "source": [
        "Next we define a helper method that does one epoch of training or evaluation. We have only defined training here, so you need to implement the necessary changes for evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfHv4_o6jq_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(model, optimizer, dataloader, train):\n",
        "    \"\"\"\n",
        "    Run one epoch of training or evaluation.\n",
        "    \n",
        "    Args:\n",
        "        model: The model used for prediction\n",
        "        optimizer: Optimization algorithm for the model\n",
        "        dataloader: Dataloader providing the data to run our model on\n",
        "        train: Whether this epoch is used for training or evaluation\n",
        "        \n",
        "    Returns:\n",
        "        Loss and accuracy in this epoch.\n",
        "    \"\"\"\n",
        "    # TODO: Change the necessary parts to work correctly during evaluation (train=False)\n",
        "    \n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Set model to training mode (for e.g. batch normalization, dropout)\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "\n",
        "    # Iterate over data\n",
        "    for xb, yb in dataloader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            pred = model(xb)\n",
        "            loss = F.cross_entropy(pred, yb)\n",
        "            top1 = torch.argmax(pred, dim=1)\n",
        "            ncorrect = torch.sum(top1 == yb)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += ncorrect.item()\n",
        "    \n",
        "    epoch_loss /= len(dataloader)\n",
        "    epoch_acc /= len(dataloader)\n",
        "    return epoch_loss, epoch_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJCecT7Bjq_p",
        "colab_type": "text"
      },
      "source": [
        "Next we implement a method for fitting (training) our model. For many models early stopping can save a lot of training time. Your task is to add early stopping to the loop (based on validation accuracy)! And don't forget to save the best model parameters according to validation accuracy. You will need `copy.deepcopy` and the `state_dict` for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4FwVyDvjq_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience):\n",
        "    \"\"\"\n",
        "    Fit the given model on the dataset.\n",
        "    \n",
        "    Args:\n",
        "        model: The model used for prediction\n",
        "        optimizer: Optimization algorithm for the model\n",
        "        lr_scheduler: Learning rate scheduler that improves training\n",
        "                      in late epochs with learning rate decay\n",
        "        dataloaders: Dataloaders for training and validation\n",
        "        max_epochs: Maximum number of epochs for training\n",
        "        patience: Number of epochs to wait with early stopping the\n",
        "                  training if validation loss has decreased\n",
        "                  \n",
        "    Returns:\n",
        "        Loss and accuracy in this epoch.g at the same time, the operation becomes significantly cheaper computationally. If you use stride=1 \n",
        "    \"\"\"\n",
        "    \n",
        "    best_acc = 0\n",
        "    curr_patience = 0\n",
        "    \n",
        "    for epoch in range(max_epochs):\n",
        "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
        "        lr_scheduler.step()\n",
        "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n",
        "        \n",
        "        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=False)\n",
        "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n",
        "        \n",
        "        # TODO: Add early stopping and save the best weights (in best_model_weights)\n",
        "    \n",
        "    model.load_state_dict(best_model_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoixnarDjq_w",
        "colab_type": "text"
      },
      "source": [
        "In most cases you should just use the Adam optimizer for training, because it works well out of the box. However, a well-tuned SGD (with momentum) will in most cases outperform Adam. And since the original paper gives us a well-tuned SGD we will just use that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1YEAcM-jq_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "48fbd121-be93-4458-a357-6dea523cee16"
      },
      "source": [
        "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
        "\n",
        "# Fit model\n",
        "fit(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=200, patience=50)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-41d6a0615e44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-81-5d376cf525b2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-48a69e7b3660>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, optimizer, dataloader, train)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-433ec9106e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# TODO: Execute the layers (blocks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_abs_string_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m_get_abs_string_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_abs_string_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;34m\"\"\"Get the absolute index for the list of modules\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index {} is out of range'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ResidualBlock' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FRlWilEjq_2",
        "colab_type": "text"
      },
      "source": [
        "Once the model is trained we run it on the test set to obtain our final accuracy.\n",
        "Note that we can only look at the test set once, everything else would lead to overfitting. So you _must_ ignore the test set while developing your model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSRzAAMzjq_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
        "print(f\"Test loss: {test_loss:.1e}, accuracy: {test_acc * 100:.2f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4LQuiS6jq__",
        "colab_type": "text"
      },
      "source": [
        "That's almost what was reported in the paper (92.49%) and we didn't even train on the full training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP8yiZ70jrAB",
        "colab_type": "text"
      },
      "source": [
        "# Optional task: Squeeze out all the juice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phWc4VKRjrAB",
        "colab_type": "text"
      },
      "source": [
        "Can you do even better? Have a look at [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/) and at the [EfficientNet architecture](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html) we discussed in the lecture. Play around with the possibilities PyTorch offers you and see how close you can get to the [state of the art on CIFAR-10](https://paperswithcode.com/sota/image-classification-on-cifar-10).\n",
        "\n",
        "Hint: You can use [Google Colab](https://colab.research.google.com/) to access some free GPUs for your experiments."
      ]
    }
  ]
}