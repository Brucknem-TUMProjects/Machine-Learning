{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the results to PDF\n",
    "\n",
    "Once you complete the assignments, export the entire notebook as PDF and attach it to your homework solutions. \n",
    "The best way of doing that is\n",
    "1. Run all the cells of the notebook.\n",
    "2. Export/download the notebook as PDF (File -> Download as -> PDF via LaTeX (.pdf)).\n",
    "3. Concatenate your solutions for other tasks with the output of Step 2. On linux, you can use `pdfunite`, there are similar tools for other platforms, too. You can only upload a single PDF file to Moodle.\n",
    "\n",
    "Make sure you are using `nbconvert` version 5.5 or later by running `jupyter nbconvert --version`. Older versions clip lines that exceed page width, which makes your code harder to grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant recommendation\n",
    "\n",
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not yet rated based on a latent factor model.\n",
    "\n",
    "Specifically, the objective function (loss) we wanted to optimize is:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(i, x) \\in W} (M_{ix} - \\mathbf{q}_i^T\\mathbf{p}_x)^2 + \\lambda\\sum_x{\\left\\lVert \\mathbf{p}_x  \\right\\rVert}^2 + \\lambda\\sum_i {\\left\\lVert\\mathbf{q}_i  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $W$ is the set of $(i, x)$ pairs for which the rating $M_{ix}$ given by user $i$ to restaurant $x$ is known. Here we have also introduced two regularization terms to help us with overfitting where $\\lambda$ is hyper-parameter that control the strength of the regularization.\n",
    "\n",
    "**Hint 1**: Using the closed form solution for regression might lead to singular values. To avoid this issue perform the regression step with an existing package such as scikit-learn. It is advisable to use ridge regression to account for regularization.\n",
    "\n",
    "**Hint 2**: If you are using the scikit-learn package remember to set ``fit_intercept = False`` to only learn the coefficients of the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Data (nothing to do here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.load(\"exercise_12_matrix_factorization_ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101968,   1880,      1],\n",
       "       [101968,    284,      5],\n",
       "       [101968,   1378,      2],\n",
       "       ...,\n",
       "       [ 72452,   2100,      4],\n",
       "       [ 72452,   2050,      5],\n",
       "       [ 74861,   3979,      5]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have triplets of (user, restaurant, rating).\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data into a matrix of dimension [N, D], where N is the number of users and D is the number of restaurants in the dataset. We store the data as a sparse matrix to avoid out-of-memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337867x5899 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 929606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = np.max(ratings[:,0] + 1)\n",
    "n_restaurants = np.max(ratings[:,1] + 1)\n",
    "M = sp.coo_matrix((ratings[:,2], (ratings[:,0], ratings[:,1])), shape=(n_users, n_restaurants)).tocsr()\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the <a href=\"https://en.wikipedia.org/wiki/Cold_start_(computing)\"> cold start problem</a>, in the preprocessing step, we recursively remove all users and restaurants with 10 or less ratings.\n",
    "\n",
    "Then, we randomly select 200 data points for the validation and test sets, respectively.\n",
    "\n",
    "After this, we subtract the mean rating for each users to account for this global effect.\n",
    "\n",
    "**Note**: Some entries might become zero in this process -- but these entries are different than the 'unknown' zeros in the matrix. We store the indices for which we the rating data available in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix which have less than min_entries nonzero entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix      : sp.spmatrix, shape [N, D]\n",
    "                  The input matrix to be preprocessed.\n",
    "    min_entries : int\n",
    "                  Minimum number of nonzero elements per row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix      : sp.spmatrix, shape [N', D']\n",
    "                  The pre-processed matrix, where N' <= N and D' <= D\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    \n",
    "    shape = (-1, -1)\n",
    "    while matrix.shape != shape:\n",
    "        shape = matrix.shape\n",
    "        nnz = matrix>0\n",
    "        row_ixs = nnz.sum(1).A1 > min_entries\n",
    "        matrix = matrix[row_ixs]\n",
    "        nnz = matrix>0\n",
    "        col_ixs = nnz.sum(0).A1 > min_entries\n",
    "        matrix = matrix[:,col_ixs]\n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    nnz = matrix>0\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement a function that subtracts the mean user rating from the sparse rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements in the input matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             Input sparse matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The modified input matrix.\n",
    "    \n",
    "    user_means : np.array, shape [N, 1]\n",
    "                 The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Compute the modified matrix and user_means\n",
    "    non_zeros = (matrix>0)\n",
    "    user_means = matrix.sum(1) / non_zeros.sum(1)\n",
    "    matrix -= sp.csr_matrix(user_means).multiply(non_zeros)\n",
    "    \n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train, validation and test set (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix          : sp.spmatrix, shape [N, D]\n",
    "                      The input data matrix.\n",
    "    n_validation    : int\n",
    "                      The number of validation entries to extract.\n",
    "    n_test          : int\n",
    "                      The number of test entries to extract.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix_split    : sp.spmatrix, shape [N, D]\n",
    "                      A copy of the input matrix in which the validation and test entries have been set to zero.\n",
    "    \n",
    "    val_idx         : tuple, shape [2, n_validation]\n",
    "                      The indices of the validation entries.\n",
    "    \n",
    "    test_idx        : tuple, shape [2, n_test]\n",
    "                      The indices of the test entries.\n",
    "    \n",
    "    val_values      : np.array, shape [n_validation, ]\n",
    "                      The values of the input matrix at the validation indices.\n",
    "                      \n",
    "    test_values     : np.array, shape [n_test, ]\n",
    "                      The values of the input matrix at the test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix_cp = matrix.copy()\n",
    "    non_zero_idx = np.argwhere(matrix_cp)\n",
    "    ixs = np.random.permutation(non_zero_idx)\n",
    "    val_idx = tuple(ixs[:n_validation].T)\n",
    "    test_idx = tuple(ixs[n_validation:n_validation + n_test].T)\n",
    "    \n",
    "    val_values = matrix_cp[val_idx].A1\n",
    "    test_values = matrix_cp[test_idx].A1\n",
    "    \n",
    "    matrix_cp[val_idx] = matrix_cp[test_idx] = 0\n",
    "    matrix_cp.eliminate_zeros()\n",
    "\n",
    "    return matrix_cp, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (337867, 5899)\n",
      "Shape after: (3529, 2072)\n"
     ]
    }
   ],
   "source": [
    "M = cold_start_preprocessing(M, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "M_train, val_idx, test_idx, val_values, test_values = split_data(M, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove user means.\n",
    "nonzero_indices = np.argwhere(M_train)\n",
    "M_shifted, user_means = shift_user_mean(M_train)\n",
    "# Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values - user_means[np.array(val_idx).T[:,0]].A1\n",
    "test_values_shifted = test_values - user_means[np.array(test_idx).T[:,0]].A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the loss function (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(values, ixs, Q, P, reg_lambda):\n",
    "    \"\"\"\n",
    "    Compute the loss of the latent factor model (at indices ixs).\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : np.array, shape [n_ixs,]\n",
    "        The array with the ground-truth values.\n",
    "    ixs : tuple, shape [2, n_ixs]\n",
    "        The indices at which we want to evaluate the loss (usually the nonzero indices of the unshifted data matrix).\n",
    "    Q : np.array, shape [N, k]\n",
    "        The matrix Q of a latent factor model.\n",
    "    P : np.array, shape [k, D]\n",
    "        The matrix P of a latent factor model.\n",
    "    reg_lambda : float\n",
    "        The regularization strength\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "           The loss of the latent factor model.\n",
    "\n",
    "    \"\"\"\n",
    "    mean_sse_loss = np.sum((values - Q.dot(P)[ixs])**2)\n",
    "    regularization_loss =  reg_lambda * (np.sum(np.linalg.norm(P, axis=0)**2) + np.sum(np.linalg.norm(Q, axis=1) ** 2))\n",
    "    \n",
    "    return mean_sse_loss + regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating optimization\n",
    "\n",
    "In the first step, we will approach the problem via alternating optimization, as learned in the lecture. That is, during each iteration you first update $Q$ while having $P$ fixed and then vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement a function that initializes the latent factors $Q$ and $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The matrix to be factorized.\n",
    "    k      : int\n",
    "             The number of latent dimensions.\n",
    "    init   : str in ['svd', 'random'], default: 'random'\n",
    "             The initialization strategy. 'svd' means that we use SVD to initialize P and Q, 'random' means we initialize\n",
    "             the entries in P and Q randomly in the interval [0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.array, shape [N, k]\n",
    "        The initialized matrix Q of a latent factor model.\n",
    "\n",
    "    P : np.array, shape [k, D]\n",
    "        The initialized matrix P of a latent factor model.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    N, D = matrix.shape\n",
    "    if init == 'svd':\n",
    "        U, s, V = svds(matrix, k=k)\n",
    "        S = np.diag(s)\n",
    "        Q = U.dot(S)\n",
    "        P = V\n",
    "    else:\n",
    "        Q = np.random.random((N, k))\n",
    "        P = np.random.random((k, D))\n",
    "        \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Implement the alternating optimization approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(M, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=5, eval_every=1):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "                      \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 5\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Q,P = initialize_Q_P(M, k, init)\n",
    "    best_Q = Q\n",
    "    best_P = P\n",
    "    best_loss = -1\n",
    "    validation_losses = []\n",
    "    train_losses = []\n",
    "    converged_after = -1\n",
    "    train_idx = tuple(non_zero_idx.T)\n",
    "    \n",
    "    reg = Ridge(alpha=reg_lambda, fit_intercept=False)\n",
    "    \n",
    "    nnz_mask = sp.coo_matrix((np.ones(len(non_zero_idx)),\n",
    "                              (non_zero_idx[:,0],non_zero_idx[:,1])),\n",
    "                             shape=M.shape, dtype=\"uint8\").tocsr()\n",
    "    rows = nnz_mask.tolil().rows\n",
    "    cols = nnz_mask.T.tolil().rows\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        if i % eval_every == 0:\n",
    "            # evaluate losses\n",
    "            val_loss = loss(val_values, val_idx, Q, P, reg_lambda)\n",
    "            validation_losses.append(val_loss)\n",
    "            \n",
    "            train_loss = loss(M[train_idx].A1, train_idx, Q, P, reg_lambda)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            if best_loss <= -1 or val_loss < best_loss:\n",
    "                best_Q = Q\n",
    "                best_P = P\n",
    "                best_loss = val_loss\n",
    "                current_patience = patience\n",
    "            else:\n",
    "                current_patience -= 1\n",
    "\n",
    "            if current_patience == 0:\n",
    "                converged_after = i - patience * eval_every\n",
    "                break  \n",
    "                \n",
    "        print(\"Iteration \", i)\n",
    "        \n",
    "        # fix Q\n",
    "        for rating_idx in range(M.shape[1]):\n",
    "            nnz_idx = cols[rating_idx]\n",
    "            res = reg.fit(Q[nnz_idx], np.squeeze(M[nnz_idx, rating_idx].toarray()))\n",
    "            P[:, rating_idx] = res.coef_\n",
    "\n",
    "        # fix P \n",
    "        for user_idx in range(M.shape[0]):\n",
    "            nnz_idx = rows[user_idx]\n",
    "            res = reg.fit(P[:, nnz_idx].T, np.squeeze(M[user_idx, nnz_idx].toarray()))\n",
    "            Q[user_idx, :] = res.coef_\n",
    "    \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the latent factor (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "Iteration  1\n",
      "Iteration  2\n",
      "Iteration  3\n",
      "Iteration  4\n",
      "Iteration  5\n",
      "Iteration  6\n",
      "Iteration  7\n",
      "Iteration  8\n",
      "Iteration  9\n",
      "Iteration  10\n",
      "Iteration  11\n",
      "Iteration  12\n",
      "Iteration  13\n",
      "Iteration  14\n",
      "Iteration  15\n",
      "Iteration  16\n",
      "Iteration  17\n",
      "Iteration  18\n",
      "Iteration  19\n",
      "Iteration  20\n",
      "Iteration  21\n",
      "Iteration  22\n",
      "Iteration  23\n",
      "Iteration  24\n",
      "Iteration  25\n",
      "Iteration  26\n",
      "Iteration  27\n",
      "Iteration  28\n",
      "Iteration  29\n",
      "Iteration  30\n",
      "Iteration  31\n",
      "Iteration  32\n",
      "Iteration  33\n",
      "Iteration  34\n",
      "Iteration  35\n",
      "Iteration  36\n",
      "Iteration  37\n",
      "Iteration  38\n",
      "Iteration  39\n",
      "Iteration  40\n",
      "Iteration  41\n",
      "Iteration  42\n",
      "Iteration  43\n",
      "Iteration  44\n",
      "Iteration  45\n",
      "Iteration  46\n",
      "Iteration  47\n",
      "Iteration  48\n",
      "Iteration  49\n",
      "Iteration  50\n",
      "Iteration  51\n",
      "Iteration  52\n",
      "Iteration  53\n",
      "Iteration  54\n",
      "Iteration  55\n",
      "Iteration  56\n",
      "Iteration  57\n",
      "Iteration  58\n",
      "Iteration  59\n",
      "Iteration  60\n",
      "Iteration  61\n",
      "Iteration  62\n",
      "Iteration  63\n",
      "Iteration  64\n",
      "Iteration  65\n",
      "Iteration  66\n",
      "Iteration  67\n",
      "Iteration  68\n",
      "Iteration  69\n",
      "Iteration  70\n",
      "Iteration  71\n",
      "Iteration  72\n",
      "Iteration  73\n",
      "Iteration  74\n",
      "Iteration  75\n",
      "Iteration  76\n",
      "Iteration  77\n",
      "Iteration  78\n",
      "Iteration  79\n",
      "Iteration  80\n",
      "Iteration  81\n",
      "Iteration  82\n",
      "Iteration  83\n",
      "Iteration  84\n",
      "Iteration  85\n",
      "Iteration  86\n",
      "Iteration  87\n",
      "Iteration  88\n",
      "Iteration  89\n",
      "Iteration  90\n",
      "Iteration  91\n",
      "Iteration  92\n",
      "Iteration  93\n",
      "Iteration  94\n",
      "Iteration  95\n",
      "Iteration  96\n",
      "Iteration  97\n",
      "Iteration  98\n",
      "Iteration  99\n"
     ]
    }
   ],
   "source": [
    "Q, P, val_loss, train_loss, converged = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                               k=100, val_idx=val_idx,\n",
    "                                                                               val_values=val_values_shifted, \n",
    "                                                                               reg_lambda=1e-4, init='random',\n",
    "                                                                               max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the validation and training losses over for each iteration (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[10, 5])\n",
    "fig.suptitle(\"Alternating optimization, k=100\")\n",
    "\n",
    "ax[0].plot(train_loss[1::])\n",
    "ax[0].set_title('Training loss')\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "ax[1].plot(val_loss[1::])\n",
    "ax[1].set_title('Validation loss')\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
